{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "185367dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import bibs\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import pyarrow\n",
    "import gc\n",
    "\n",
    "from functools import reduce\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "path = 'data/train_data'\n",
    "path_to_save = 'data/preprocess_train_data'\n",
    "path_final = 'data/process_data'\n",
    "\n",
    "columns_full = ['id',\n",
    "                'rn',\n",
    "                'pre_since_opened',\n",
    "                'pre_since_confirmed',\n",
    "                'pre_pterm',\n",
    "                'pre_fterm',\n",
    "                'pre_till_pclose',\n",
    "                'pre_till_fclose',\n",
    "                'pre_loans_credit_limit',\n",
    "                'pre_loans_next_pay_summ',\n",
    "                'pre_loans_outstanding',\n",
    "                'pre_loans_total_overdue',\n",
    "                'pre_loans_max_overdue_sum',\n",
    "                'pre_loans_credit_cost_rate',\n",
    "                'pre_loans5',\n",
    "                'pre_loans530',\n",
    "                'pre_loans3060',\n",
    "                'pre_loans6090',\n",
    "                'pre_loans90',\n",
    "                'is_zero_loans5',\n",
    "                'is_zero_loans530',\n",
    "                'is_zero_loans3060',\n",
    "                'is_zero_loans6090',\n",
    "                'is_zero_loans90',\n",
    "                'pre_util',\n",
    "                'pre_over2limit',\n",
    "                'pre_maxover2limit',\n",
    "                'is_zero_util',\n",
    "                'is_zero_over2limit',\n",
    "                'is_zero_maxover2limit',\n",
    "                'enc_paym_0',\n",
    "                'enc_paym_1',\n",
    "                'enc_paym_2',\n",
    "                'enc_paym_3',\n",
    "                'enc_paym_4',\n",
    "                'enc_paym_5',\n",
    "                'enc_paym_6',\n",
    "                'enc_paym_7',\n",
    "                'enc_paym_8',\n",
    "                'enc_paym_9',\n",
    "                'enc_paym_10',\n",
    "                'enc_paym_11',\n",
    "                'enc_paym_12',\n",
    "                'enc_paym_13',\n",
    "                'enc_paym_14',\n",
    "                'enc_paym_15',\n",
    "                'enc_paym_16',\n",
    "                'enc_paym_17',\n",
    "                'enc_paym_18',\n",
    "                'enc_paym_19',\n",
    "                'enc_paym_20',\n",
    "                'enc_paym_21',\n",
    "                'enc_paym_22',\n",
    "                'enc_paym_23',\n",
    "                'enc_paym_24',\n",
    "                'enc_loans_account_holder_type',\n",
    "                'enc_loans_credit_status',\n",
    "                'enc_loans_credit_type',\n",
    "                'enc_loans_account_cur',\n",
    "                'pclose_flag',\n",
    "                'fclose_flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25c6ae8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'rn',\n",
       " 'pre_since_opened',\n",
       " 'pre_since_confirmed',\n",
       " 'pre_pterm',\n",
       " 'pre_fterm',\n",
       " 'pre_till_pclose',\n",
       " 'pre_till_fclose',\n",
       " 'pre_loans_credit_limit',\n",
       " 'pre_loans_next_pay_summ',\n",
       " 'pre_loans_outstanding',\n",
       " 'pre_loans_total_overdue',\n",
       " 'pre_loans_max_overdue_sum',\n",
       " 'pre_loans_credit_cost_rate',\n",
       " 'pre_loans5',\n",
       " 'pre_loans530',\n",
       " 'pre_loans3060',\n",
       " 'pre_loans6090',\n",
       " 'pre_loans90',\n",
       " 'is_zero_loans5',\n",
       " 'is_zero_loans530',\n",
       " 'is_zero_loans3060']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['id', 'rn'] + columns_full[2:22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2464f015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parquet_dataset_from_local(path_to_dataset: str, start_from: int = 0,\n",
    "                                    num_parts_to_read: int = 4, columns=None, verbose=False):\n",
    "    res = []\n",
    "    dataset_paths = sorted([os.path.join(path_to_dataset, filename) for filename in os.listdir(path_to_dataset)\n",
    "                            if filename.startswith('train')])\n",
    "    print(dataset_paths)\n",
    "\n",
    "    start_from = max(0, start_from)\n",
    "    chunks = dataset_paths[start_from: start_from + num_parts_to_read]\n",
    "\n",
    "    if verbose:\n",
    "        print('Reading chunks!')\n",
    "        for chunk in chunks:\n",
    "            print('Read chunk:', chunk)\n",
    "\n",
    "    for chunk_path in tqdm.tqdm_notebook(chunks, desc='Reading chunks with pandas!'):\n",
    "        print('Chunk', chunk_path)\n",
    "        res.append(pd.read_parquet(chunk_path, columns=columns))\n",
    "\n",
    "    return pd.concat(res).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbb77ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_aggregator(data_frame: pd.DataFrame) -> pd.DataFrame:\n",
    "    features = list(data_frame.columns.values)\n",
    "    features.remove('id')\n",
    "    features.remove('rn')\n",
    "\n",
    "    enc_list = ['id', 'rn']\n",
    "\n",
    "    dummies = pd.get_dummies(data_frame[features], columns=features)\n",
    "    dummy_features = dummies.columns.values\n",
    "    data_frame = data_frame[enc_list]\n",
    "\n",
    "    ohe_features = pd.concat([data_frame, dummies], axis=1)\n",
    "\n",
    "    features_output = ohe_features.groupby('id')[dummy_features].sum().reset_index(drop=False)\n",
    "\n",
    "    return features_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1af0f6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_aggregator(data_frame: pd.DataFrame) -> pd.DataFrame:\n",
    "    features = list(data_frame.columns.values)\n",
    "    features.remove('id')\n",
    "    features.remove('rn')\n",
    "\n",
    "    enc_list = ['id', 'rn']\n",
    "\n",
    "    dummies = pd.get_dummies(data_frame[features], columns=features)\n",
    "    dummy_features = dummies.columns.values\n",
    "    data_frame = data_frame[enc_list]\n",
    "    \n",
    "    ohe_features = pd.concat([data_frame, dummies], axis=1)\n",
    "\n",
    "    history_lenght = ohe_features.groupby('id')['rn'].max().reset_index(drop=False).rename(\n",
    "        columns={'rn': 'history_lenght'})\n",
    "\n",
    "    ohe_features = ohe_features.merge(history_lenght, on='id')\n",
    "\n",
    "    ohe_features['weight'] = ((ohe_features['rn'] / ohe_features['history_lenght']) ** 1.4).round(4)\n",
    "    sum_weights = ohe_features.groupby('id')['weight'].sum().reset_index(drop=False).rename(\n",
    "        columns={'weight': 'sum_weights'})\n",
    "    ohe_features = ohe_features.merge(sum_weights, on='id')\n",
    "\n",
    "    result_features = list(ohe_features.columns.values)\n",
    "    result_features.remove('id')\n",
    "    result_features.remove('rn')\n",
    "    result_features.remove('history_lenght')\n",
    "    result_features.remove('weight')\n",
    "    result_features.remove('sum_weights')\n",
    "\n",
    "    for feature in result_features:\n",
    "        ohe_features[feature] = (ohe_features[feature] * ohe_features['weight'] / ohe_features['sum_weights']).round(4)\n",
    "\n",
    "    return ohe_features.groupby('id')[result_features].sum().reset_index(drop=False).merge(history_lenght, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a28f2bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_transactions_dataset_count(path_to_dataset: str, num_parts_to_preprocess_at_once: int = 1,\n",
    "                                       num_parts_total: int = 12,\n",
    "                                       save_to_path=None, verbose: bool = False, columns_full=None):\n",
    "    preprocessed_frames = []\n",
    "\n",
    "    for step in tqdm.tqdm_notebook(range(0, num_parts_total, num_parts_to_preprocess_at_once),\n",
    "                                   desc='Transforming transactions data'):\n",
    "        transactions_frame = read_parquet_dataset_from_local(path_to_dataset, step, num_parts_to_preprocess_at_once,\n",
    "                                                             verbose=verbose)\n",
    "        transactions_frame_list = []\n",
    "        \n",
    "        \n",
    "        for i in range(2):\n",
    "            if i == 0:\n",
    "                columns = ['id', 'rn'] + columns_full[2:22]\n",
    "                transactions_frame_part = transactions_frame[columns]\n",
    "                transactions_frame_part = count_aggregator(transactions_frame_part)\n",
    "                transactions_frame_list.append(transactions_frame_part)\n",
    "\n",
    "            else:\n",
    "                columns = ['id', 'rn'] + columns_full[22:]\n",
    "                transactions_frame_part = transactions_frame[columns]\n",
    "                transactions_frame_part = count_aggregator(transactions_frame_part)\n",
    "                transactions_frame_list.append(transactions_frame_part)\n",
    "\n",
    "        transactions_frame = transactions_frame_list[0].merge(transactions_frame_list[1], how='inner', on='id')\n",
    "\n",
    "        if save_to_path:\n",
    "            block_as_str = str(step)\n",
    "            if len(block_as_str) == 1:\n",
    "                block_as_str = '00' + block_as_str\n",
    "\n",
    "            else:\n",
    "                block_as_str = '0' + block_as_str\n",
    "\n",
    "            transactions_frame.to_parquet(\n",
    "                os.path.join(path_to_save, f'processed_chunk_count_agg_{block_as_str}.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36e4825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_transactions_dataset_weight(path_to_dataset: str, num_parts_preprocess_at_once: int = 1,\n",
    "                                        num_parts_total: int = 12,\n",
    "                                        save_to_path=None, verbose: bool = False, columns_full=None):\n",
    "    preprocessed_frames = []\n",
    "\n",
    "    for step in tqdm.tqdm_notebook(range(0, num_parts_total, num_parts_preprocess_at_once),\n",
    "                                   desc='Transforming transactions data'):\n",
    "        transactions_frame = read_parquet_dataset_from_local(path_to_dataset, step, num_parts_preprocess_at_once,\n",
    "                                                             verbose=verbose)\n",
    "        \n",
    "\n",
    "        transactions_frame_list = []\n",
    "\n",
    "        for i in range(16):\n",
    "            if i == 0:\n",
    "                columns = ['id', 'rn'] + columns_full[2:6]\n",
    "                transactions_frame_part = transactions_frame[columns]\n",
    "                transactions_frame_part = weight_aggregator(transactions_frame_part)\n",
    "                transactions_frame_list.append(transactions_frame_part.drop('history_lenght', axis=1))\n",
    "                \n",
    "                \n",
    "            elif i == 1:\n",
    "                columns = ['id', 'rn'] + columns_full[6:10]\n",
    "                transactions_frame_part = transactions_frame[columns]\n",
    "                transactions_frame_part = weight_aggregator(transactions_frame_part)\n",
    "                transactions_frame_list.append(transactions_frame_part.drop('history_lenght', axis=1))\n",
    "\n",
    "            elif i == 2:\n",
    "                columns = ['id', 'rn'] + columns_full[10:14]\n",
    "                transactions_frame_part = transactions_frame[columns]\n",
    "                transactions_frame_part = weight_aggregator(transactions_frame_part)\n",
    "                transactions_frame_list.append(transactions_frame_part.drop('history_lenght', axis=1))\n",
    "\n",
    "            elif i == 3:\n",
    "                columns = ['id', 'rn'] + columns_full[14:18]\n",
    "                transactions_frame_part = transactions_frame[columns]\n",
    "                transactions_frame_part = weight_aggregator(transactions_frame_part)\n",
    "                transactions_frame_list.append(transactions_frame_part.drop('history_lenght', axis=1))\n",
    "\n",
    "            elif i == 4:\n",
    "                columns = ['id', 'rn'] + columns_full[18:22]\n",
    "                transactions_frame_part = transactions_frame[columns]\n",
    "                transactions_frame_part = weight_aggregator(transactions_frame_part)\n",
    "                transactions_frame_list.append(transactions_frame_part.drop('history_lenght', axis=1))\n",
    "\n",
    "            elif i == 5:\n",
    "                columns = ['id', 'rn'] + columns_full[22:26]\n",
    "                transactions_frame_part = transactions_frame[columns]\n",
    "                transactions_frame_part = weight_aggregator(transactions_frame_part)\n",
    "                transactions_frame_list.append(transactions_frame_part.drop('history_lenght', axis=1))\n",
    "\n",
    "            elif i == 6:\n",
    "                columns = ['id', 'rn'] + columns_full[26:30]\n",
    "                transactions_frame_part = transactions_frame[columns]\n",
    "                transactions_frame_part = weight_aggregator(transactions_frame_part)\n",
    "                transactions_frame_list.append(transactions_frame_part.drop('history_lenght', axis=1))\n",
    "\n",
    "            elif i == 7:\n",
    "                columns = ['id', 'rn'] + columns_full[30:34]\n",
    "                transactions_frame_part = transactions_frame[columns]\n",
    "                transactions_frame_part = weight_aggregator(transactions_frame_part)\n",
    "                transactions_frame_list.append(transactions_frame_part.drop('history_lenght', axis=1))\n",
    "\n",
    "            elif i == 8:\n",
    "                columns = ['id', 'rn'] + columns_full[34:38]\n",
    "                transactions_frame_part = transactions_frame[columns]\n",
    "                transactions_frame_part = weight_aggregator(transactions_frame_part)\n",
    "                transactions_frame_list.append(transactions_frame_part.drop('history_lenght', axis=1))\n",
    "\n",
    "            elif i == 9:\n",
    "                columns = ['id', 'rn'] + columns_full[38:41]\n",
    "                transactions_frame_part = transactions_frame[columns]\n",
    "                transactions_frame_part = weight_aggregator(transactions_frame_part)\n",
    "                transactions_frame_list.append(transactions_frame_part.drop('history_lenght', axis=1))\n",
    "\n",
    "            elif i == 10:\n",
    "                columns = ['id', 'rn'] + columns_full[41:44]\n",
    "                transactions_frame_part = transactions_frame[columns]\n",
    "                transactions_frame_part = weight_aggregator(transactions_frame_part)\n",
    "                transactions_frame_list.append(transactions_frame_part.drop('history_lenght', axis=1))\n",
    "\n",
    "            elif i == 11:\n",
    "                columns = ['id', 'rn'] + columns_full[44:47]\n",
    "                transactions_frame_part = transactions_frame[columns]\n",
    "                transactions_frame_part = weight_aggregator(transactions_frame_part)\n",
    "                transactions_frame_list.append(transactions_frame_part.drop('history_lenght', axis=1))\n",
    "                \n",
    "            elif i == 12:\n",
    "                columns = ['id', 'rn'] + columns_full[47:50]\n",
    "                transactions_frame_part = transactions_frame[columns]\n",
    "                transactions_frame_part = weight_aggregator(transactions_frame_part)\n",
    "                transactions_frame_list.append(transactions_frame_part.drop('history_lenght', axis=1))\n",
    "\n",
    "            elif i == 13:\n",
    "                columns = ['id', 'rn'] + columns_full[50:53]\n",
    "                transactions_frame_part = transactions_frame[columns]\n",
    "                transactions_frame_part = weight_aggregator(transactions_frame_part)\n",
    "                transactions_frame_list.append(transactions_frame_part.drop('history_lenght', axis=1))\n",
    "\n",
    "            elif i == 14:\n",
    "                columns = ['id', 'rn'] + columns_full[53:56]\n",
    "                transactions_frame_part = transactions_frame[columns]\n",
    "                transactions_frame_part = weight_aggregator(transactions_frame_part)\n",
    "                transactions_frame_list.append(transactions_frame_part.drop('history_lenght', axis=1))\n",
    "\n",
    "            else:\n",
    "                columns = ['id', 'rn'] + columns_full[56:]\n",
    "                transactions_frame_part = transactions_frame[columns]\n",
    "                transactions_frame_part = weight_aggregator(transactions_frame_part)\n",
    "                transactions_frame_list.append(transactions_frame_part)\n",
    "\n",
    "        transactions_frame = reduce(lambda left, right: pd.merge(left, right, on='id', how='inner'),\n",
    "                                    transactions_frame_list)\n",
    "\n",
    "        if save_to_path:\n",
    "            block_as_str = str(step)\n",
    "            if len(block_as_str) == 1:\n",
    "                block_as_str = '00' + block_as_str\n",
    "\n",
    "            else:\n",
    "                block_as_str = '0' + block_as_str\n",
    "\n",
    "            transactions_frame.to_parquet(\n",
    "                os.path.join(save_to_path, f'processed_chunk_weight_agg_{block_as_str}.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f379351",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_preprocessing_data(path_to_dataset: str, target_path: str, start_from: int = 0,\n",
    "                              num_parts_to_read: int = 1, verbose: bool = False,\n",
    "                              columns=None) -> pd.DataFrame:\n",
    "    res_count = []\n",
    "    res_weight = []\n",
    "\n",
    "    dataset_paths_count = sorted([os.path.join(path_to_dataset, filename) for filename in os.listdir(path_to_dataset)\n",
    "                                  if filename.startswith('processed_chunk_count')])\n",
    "\n",
    "    dataset_paths_weight = sorted([os.path.join(path_to_dataset, filename) for filename in os.listdir(path_to_dataset)\n",
    "                                   if filename.startswith('processed_chunk_weight')])\n",
    "\n",
    "    print(dataset_paths_count)\n",
    "    start_from = max(0, start_from)\n",
    "\n",
    "    chunks_count = dataset_paths_count[start_from: start_from + num_parts_to_read]\n",
    "    chunks_weight = dataset_paths_weight[start_from: start_from + num_parts_to_read]\n",
    "\n",
    "    if verbose:\n",
    "        print('Reading chunks!')\n",
    "\n",
    "    for chunk in tqdm.tqdm_notebook(chunks_count, desc='Reading chunks in COUNT frames'):\n",
    "        res_count.append(pd.read_parquet(chunk, columns=columns))\n",
    "\n",
    "    for chunk in tqdm.tqdm_notebook(chunks_weight, desc='Reading chunks in WEIGHT frames'):\n",
    "        res_weight.append(pd.read_parquet(chunk, columns=columns))\n",
    "\n",
    "    res_frame = pd.concat(res_count).merge(pd.concat(res_weight), how='inner', on='id')\n",
    "    res_frame = res_frame.fillna(np.uint8(0))\n",
    "    target = pd.read_csv(os.path.join(target_path, 'train_target.csv'))\n",
    "    res_frame = res_frame.merge(target, on='id')\n",
    "\n",
    "    res_frame.to_csv(os.path.join(path_final, 'train_data_w_target.csv'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be0b01a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_data(path_to_dataset=path, num_parts_to_preprocess_at_once=1, num_parts_total=12,\n",
    "                save_to_path=path_to_save, verbose=True, columns_full=columns_full, target_path=path_final):\n",
    "    \n",
    "    print('Modify data!')\n",
    "    \n",
    "    prepare_transactions_dataset_count(path_to_dataset=path_to_dataset,\n",
    "                                       num_parts_to_preprocess_at_once=num_parts_to_preprocess_at_once,\n",
    "                                       num_parts_total=num_parts_total,\n",
    "                                       save_to_path=save_to_path,\n",
    "                                       verbose=verbose,\n",
    "                                       columns_full=columns_full)\n",
    "    \n",
    "    gc.collect()\n",
    "        \n",
    "    prepare_transactions_dataset_weight(path_to_dataset=path_to_dataset,\n",
    "                                       num_parts_preprocess_at_once=num_parts_to_preprocess_at_once,\n",
    "                                       num_parts_total=num_parts_total,\n",
    "                                       save_to_path=save_to_path,\n",
    "                                       verbose=verbose,\n",
    "                                       columns_full=columns_full)\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    concat_preprocessing_data(path_to_dataset=path_to_save,\n",
    "                              target_path=target_path,\n",
    "                              start_from=0,\n",
    "                              num_parts_to_read=12,\n",
    "                              verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
